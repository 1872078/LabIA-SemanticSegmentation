# -*- coding: utf-8 -*-
"""00 - LAB-AI_PROJECT_DEF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VU-j6yjROHROeFgAGvDoYL91E7h9kmxh

# Semantic Segmentation su dataset DynamicEarthNet

La segmentazione semantica è un algoritmo di deep learning che associa un'etichetta o una categoria a ogni pixel di un'immagine.

## Passo 1. Preparazione dell'ambiente

###### Montaggio Google Drive
"""

# This mounts Google Drive to the Colab VM.
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Enter the foldername in your Drive where you have saved the unzipped dataset folders
FOLDERNAME = "Project"
assert FOLDERNAME is not None, "[!] Enter the foldername."

# This ensures that the Python interpreter of the Colab VM can load python files from within it.
import sys
sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))

"""###### Import librerie"""

import torch
import random
import numpy as np
import os
from tqdm import tqdm
try:
    import rasterio
except ModuleNotFoundError:
    !pip install --quiet rasterio
    import rasterio
import matplotlib.pyplot as plt
import torchvision.transforms as transforms
from torchvision.transforms.functional import InterpolationMode, hflip, vflip, rotate
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import confusion_matrix
import seaborn as sns

"""###### Device setting"""

# Check if CUDA (GPU) is available, otherwise use CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device: ", device)

"""###### Seed per riproducibilità"""

def set_seed(seed):
    random.seed = seed
    np.random.seed = seed
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    print(f"SEED{seed} impostato")

set_seed(42)

"""###### Percorso file progetto"""

ROOT = "./drive/MyDrive/Project"

# Paths for checkpoints
base_model_checkpoint_path = os.path.join(ROOT, "base_checkpoints")
base_best_checkpoint_path = os.path.join(ROOT, "base_best_model/best_unet_segmentation_model.pth")
base_saved_model_path = os.path.join(ROOT, "base_saved_model/unet_segmentation_model.pth")

aug_model_checkpoint_path = os.path.join(ROOT, "aug_checkpoints")
aug_best_checkpoint_path = os.path.join(ROOT, "aug_best_model/best_unet_segmentation_model.pth")
aug_saved_model_path = os.path.join(ROOT, "aug_saved_model/unet_segmentation_model.pth")

model_checkpoint_path = os.path.join(ROOT, "final_checkpoints")
best_checkpoint_path = os.path.join(ROOT, "final_best_model/best_unet_segmentation_model.pth")
saved_model_path = os.path.join(ROOT, "final_saved_model/unet_segmentation_model.pth")

"""## Passo 2. Analisi dei dati

In seguito, quando verranno calcolati i pesi da utilizzare con la funzione di loss, noteremo uno sbilanciamento nei dati. Questo significa che alcune classi hanno molti meno campioni di addestramento rispetto ad altre classi. Per gestire questa disuguaglianza, verranno assegnati pesi diversi alle classi nella funzione di loss per garantire che il modello apprenda in modo equilibrato da tutte le classi.

###### Classi
"""

CLASS_NAMES = ['imp. surf.',    # impervious surface
               'agric.',        # agriculture
               'veget.',        # forest & other vegetation
               'wet',           # wetlands
               'soil',          # soil
               'water',         # water
               'snow & ice']    # snow & ice

"""###### Calcolo di MEAN e STD"""

# MEAN and STD computed in next cell
MEAN = [666.90438767, 903.53696712, 1021.31235693, 2610.93005398]
STD = [663.93310948, 778.57368977, 991.68901418, 1052.51984503]

# Image channels
NUM_BANDS = 4

# Train files split path
train_path = os.path.join(ROOT, "temp/train.txt")

if MEAN is None and STD is None:
    # std_sum = np.zeros(num_bands)
    num_pixels = 0

    # Train files opening
    with open(train_path, 'r') as f:
        train_files_list = [line.rstrip().split(' ') for line in f]
    train_files, _ = list(zip(*train_files_list))

    # MEAN
    print("Computing the mean...")
    sum = np.zeros((NUM_BANDS,))
    for image_path in tqdm(train_files, leave=False):
        image_path = os.path.join(ROOT, image_path)
        with rasterio.open(image_path) as src:
            img = src.read()
            sum += np.sum(img, axis=(1, 2))
            num_pixels += img.shape[1] * img.shape[2]

    MEAN = sum / num_pixels
    print("MEAN: ", MEAN)

    # STD
    print("Computing the standard deviation...")
    sum = np.zeros((NUM_BANDS,))
    for image_path in tqdm(train_files, leave=False):
        image_path = os.path.join(ROOT, image_path)
        with rasterio.open(image_path) as src:
            img = src.read()
            sum += np.sum((img - MEAN.reshape(NUM_BANDS, 1, 1)) ** 2, axis=(1, 2))

    STD = np.sqrt(sum / num_pixels)
    print("STD: ", STD)

"""###### Visualizzazione canali delle immagini in grayscale"""

with open(train_path, 'r') as f:
    train_files_list = [line.rstrip().split(' ') for line in f]
train_files, train_labels = list(zip(*train_files_list))

for i in range(0, len(train_files_list), 180):
    # Load the 4 bands into 2d arrays - band order is BGRN
    img = rasterio.open(os.path.join(ROOT, train_files[i])).read()
    img = img.astype(np.float32)

    fig, axes = plt.subplots(1, 4, figsize=(20, 5))

    # Displaying the blue band
    img_blue = img[0]
    axes[0].imshow(img_blue, cmap="gray")
    axes[0].set_title("Blue Channel")

    # Displaying the green band
    img_green = img[1]
    axes[1].imshow(img_green, cmap="gray")
    axes[1].set_title("Green Channel")

    # Displaying the red band
    img_red = img[2]
    axes[2].imshow(img_red, cmap="gray")
    axes[2].set_title("Red Channel")

    # Displaying the infrared band
    img_nir = img[3]
    axes[3].imshow(img_nir, cmap="gray")
    axes[3].set_title("NIR Channel")

    plt.suptitle("Image Channels")

    plt.show()

"""###### Visualizzazione immagini in RGB"""

# Attempt to visualize images using rasterio
for i in range(0, len(train_files_list), 180):
    # Load the 4 bands into 2d arrays - band order is BGRN
    img = rasterio.open(os.path.join(ROOT, train_files[i])).read()

    # Calculate the lower and upper intensity values based on percentiles
    lower_percentile = 5   # Adjust this value as needed
    upper_percentile = 95  # Adjust this value as needed

    lower_value = np.percentile(img, lower_percentile)
    upper_value = np.percentile(img, upper_percentile)

    # Clip the image based on intensity values
    img = np.clip(img, lower_value, upper_value)

    # Normalize the image
    img = (img - np.min(img)) / (np.max(img) - np.min(img))

    # Convert to 8-bit uint
    img = (img * 255).astype(np.uint8)

    # Transpose and rearrange color channels
    img = img.transpose(1, 2, 0)
    img = img[:, :, [2, 1, 0]]  # BGR to RGB

    # Display the image using matplotlib
    plt.imshow(img)
    plt.axis('off')  # Turn off axis labels and ticks
    plt.show()

"""## Passo 3. Creazione classe del dataset

###### Trasformazioni
"""

def undo_normalize_scale(img):
    mean = MEAN
    std = STD
    img = img * std + mean

    # Calculate the lower and upper intensity values based on percentiles
    # A percentile is the value of a (random) variable below which a certain percentage
    # of observations falls. For example, the 10th percentile is the value below which
    # 10% of the observations are located.
    lower_percentile = 5   # Adjust this value as needed
    upper_percentile = 95  # Adjust this value as needed

    lower_value = np.percentile(img, lower_percentile)
    upper_value = np.percentile(img, upper_percentile)

    # Clip the image based on intensity values
    img = np.clip(img, lower_value, upper_value)

    # Normalize the image
    img = (img - np.min(img)) / (np.max(img) - np.min(img))

    # Convert to 8-bit uint
    img = (img * 255).astype(np.uint8)
    return img

# Nearest neighbor interpolation is a type of interpolation.
# This method simply determines the “nearest” neighboring pixel and assumes its intensity value
train_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize(512, InterpolationMode.NEAREST)
])

val_test_transform = transforms.ToTensor()

"""###### Classe Dataset"""

class DynamicEarthNet(Dataset):
    def __init__(self, mode, augmentation=True):
        self.mean = MEAN
        self.std = STD

        self.aug = augmentation

        self.root = ROOT
        self.num_classes = 7

        self.mode = mode

        self.normalize = transforms.Normalize(self.mean, self.std)
        self.transform = train_transform if mode == "train" else val_test_transform

        data_path = os.path.join(self.root, f"temp/{self.mode}.txt")

        with open(data_path, 'r') as f:
            file_list = [line.rstrip().split(' ') for line in f]
        self.files, self.labels = list(zip(*file_list))

    def __len__(self):
        return len(self.files)

    def random_flip(self, img, mask):
        # Random horizontal flip
        if np.random.rand() < 0.5:
            img = hflip(img)
            mask = hflip(mask)

        # Random vertical flip
        if np.random.rand() < 0.5:
            img = vflip(img)
            mask = vflip(mask)

        return img, mask

    def random_rotate(self, img, mask):
        # Random rotation
        if np.random.rand() < 0.5:
            img = rotate(img, 90)
            mask = rotate(mask, 90)

        return img, mask

    def __getitem__(self, index):
        # Load image
        img = rasterio.open(os.path.join(self.root, self.files[index]))
        blue = img.read(1)
        green = img.read(2)
        red = img.read(3)
        nir = img.read(4)
        img = np.dstack((blue, green, red, nir))
        img = img.astype(np.float32)

        # Load label
        label = rasterio.open(os.path.join(self.root, self.labels[index])).read()
        mask = np.zeros((label.shape[1], label.shape[2]), dtype=np.int64)
        for i in range(self.num_classes):
            if i == 6:
                mask[label[i, :, :] == 255] = -1
            else:
                mask[label[i, :, :] == 255] = i

        if self.mode == "train":
            img = self.transform(img)
            mask = self.transform(mask)
            if self.aug:
                img, mask = self.random_flip(img, mask)
                img, mask = self.random_rotate(img, mask)
            img = self.normalize(img)
        elif self.mode == "test" or self.mode == "val":
            img = self.transform(img)
            mask = self.transform(mask)
            img = self.normalize(img)
        else:
            raise ValueError(f"Mode {self.mode} not recognized")

        mask = mask.long()
        mask = mask.squeeze()
        return img, mask

"""## Passo 4. Creazione DataLoader e calcolo pesi

###### DataLoader
"""

no_aug_train_set = DynamicEarthNet("train", augmentation=False)
no_aug_train_loader = DataLoader(no_aug_train_set, batch_size=4, shuffle=True, pin_memory=True, drop_last=True)
print("no_aug_train_set: ", no_aug_train_set[0])
no_aug_train_img, no_aug_train_lab = no_aug_train_set[0]
print(f"No Aug Train set has {len(no_aug_train_set)} images")
print("No Aug Train images shape: ", no_aug_train_img.shape)
print("No Aug Train labels shape: ", no_aug_train_lab.shape)

train_set = DynamicEarthNet("train")
train_loader = DataLoader(train_set, batch_size=4, shuffle=True, pin_memory=True, drop_last=True)
print("train_set: ", train_set[0])
train_img, train_lab = train_set[0]
print(f"Train set has {len(train_set)} images")
print("Train images shape: ", train_img.shape)
print("Train labels shape: ", train_lab.shape)

val_set = DynamicEarthNet("val")
val_loader = DataLoader(val_set, batch_size=4, shuffle=False, drop_last=False)
print("val_set: ", val_set[0])
val_img, val_lab = val_set[0]
print(f"Validation set has {len(val_set)} images")
print("Validation images shape: ", val_img.shape)
print("Validation labels shape: ", val_lab.shape)

test_set = DynamicEarthNet("test")
test_loader = DataLoader(test_set, batch_size=4, shuffle=False, drop_last=False)
print("test_set: ", test_set[0])
test_img, test_lab = test_set[0]
print(f"Test set has {len(test_set)} images")
print("Test images shape: ", test_img.shape)
print("Test labels shape: ", test_lab.shape)

"""###### Calcolo pesi per CrossEntropyLoss"""

# Number of pixels for each class computed in next cell
pixel_tot = 283115520
pixel_0 = 20786192
pixel_1 = 26038641
pixel_2 = 136609918
pixel_3 = 1045248
pixel_4 = 75873605
pixel_5 = 18084781
pixel_6 = 4677135

# Weights calculation with number of pixels analysis
if pixel_tot is None:
    pixel_tot = len(train_set) * 512 * 512  # for resized images with augmentations
    pixel_0 = 0
    pixel_1 = 0
    pixel_2 = 0
    pixel_3 = 0
    pixel_4 = 0
    pixel_5 = 0
    pixel_6 = 0

    for batch in tqdm(train_loader, leave=False):
        images, labels = batch

        for i in range(len(images)):
            img = images[i]
            label = labels[i]

            pixel_0 += (label==0).sum().item()
            pixel_1 += (label==1).sum().item()
            pixel_2 += (label==2).sum().item()
            pixel_3 += (label==3).sum().item()
            pixel_4 += (label==4).sum().item()
            pixel_5 += (label==5).sum().item()
            pixel_6 += (label==-1).sum().item()

            print(label)
            print('\n')

            print(img)
            print('\n')

    print("Pixel Totali: ", pixel_tot)
    print("Pixel Classe 0: ", pixel_0)
    print("Pixel Classe 1: ", pixel_1)
    print("Pixel Classe 2: ", pixel_2)
    print("Pixel Classe 3: ", pixel_3)
    print("Pixel Classe 4: ", pixel_4)
    print("Pixel Classe 5: ", pixel_5)
    print("Pixel Classe 6: ", pixel_6)

# Weights for loss function
w0 = pixel_0 / pixel_tot
w1 = pixel_1 / pixel_tot
w2 = pixel_2 / pixel_tot
w3 = pixel_3 / pixel_tot
w4 = pixel_4 / pixel_tot
w5 = pixel_5 / pixel_tot
w6 = pixel_6 / pixel_tot

"""###### Visualizzazione immagini dal DataLoader"""

# Display some images
num_images_to_display = 5

# Set up the figure
fig, axes = plt.subplots(nrows=1, ncols=num_images_to_display, figsize=(15, 15))

# Randomly select images to display
indices = np.random.choice(len(train_set), size=num_images_to_display, replace=False)

# Iterate over selected indices and display images
for i, idx in enumerate(indices):
    image, label = train_set[idx]
    image_np = undo_normalize_scale(image.numpy().transpose(1, 2, 0))
    image_np = image_np[:, :, [2, 1, 0]]    # BGR => RGB
    axes[i].imshow(image_np)    # Display the RGB image
    axes[i].set_title(f"Image {idx}")
    print(f"Image {idx}: {image.shape}")
    print(f"Label {idx}: {label.shape}")

# Adjust spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

"""## Passo 5. Definizione metriche

###### Intersection Over Union

**Pixel-wise IoU (Intersection over Union):** Questa metrica calcola il rapporto tra l'intersezione delle maschere predette e delle maschere reali e l'unione di tali maschere. È una metrica per immagine e fornisce informazioni sull'accuratezza spaziale delle predizioni.
"""

def compute_intersection_over_union(pred, label, num_classes=6):
    pred = np.asarray(pred) + 1
    label = np.asarray(label) + 1
    pred = pred * (label > 0)    # mask out ignored class (-1)

    intersection = pred * (pred == label)
    area_inter, _ = np.histogram(intersection, bins=num_classes, range=(1, num_classes))
    area_pred, _ = np.histogram(pred, bins=num_classes, range=(1, num_classes))
    area_label, _ = np.histogram(label, bins=num_classes, range=(1, num_classes))
    area_union = area_pred + area_label - area_inter

    return area_inter, area_union

def compute_iou(intersection, union):
    iou = 1.0 * intersection / (np.spacing(1) + union)
    return iou

"""###### Pixel Accuracy

**Accuracy:** misura la percentuale di pixel correttamente classificati rispetto al totale dei pixel nell'immagine
"""

def compute_pixel_accuracy(output, target):
    output = np.asarray(output)
    target = np.asarray(target)
    pixel_labeled = np.sum(target >= 0)
    pixel_correct = np.sum((output == target) * (target >= 0))

    return pixel_correct, pixel_labeled

def compute_pix_acc(pixel_correct, pixel_labeled):
    pix_acc = 1.0 * pixel_correct / (np.spacing(1) + pixel_labeled)
    return pix_acc

"""###### Intersection Over Union from Confusion Matrix

**IoU from Confusion Matrix:** Questa metrica calcola l'IoU per ciascuna classe considerando la matrice di confusione, che riassume le prestazioni di classificazione in tutte le immagini nel set di validazione. Considera i conteggi dei true positives, dei false positives e dei false negatives per ciascuna classe. Questa metrica fornisce una visione più globale della capacità del modello di classificare correttamente diverse classi.
"""

def confusion_iou(confusion_matrix):
    true_pos = np.diag(confusion_matrix)
    false_pos = np.sum(confusion_matrix, axis = 0) - true_pos
    false_neg = np.sum(confusion_matrix, axis = 1) - true_pos

    intersection = true_pos
    union = true_pos + false_pos + false_neg

    iou = intersection / union
    return iou

"""## Passo 6. Creazione del modello U-Net

###### Blocchi

**Convoluzione:** La convoluzione di immagini per la segmentazione semantica è un processo fondamentale che consiste nell'applicare filtri a un'immagine per rilevare e catturare caratteristiche significative. Questo aiuta un modello a distinguere e classificare diverse parti dell'immagine in base al loro significato. I risultati della convoluzione vengono utilizzati per creare mappe di caratteristiche, che poi guidano il processo di assegnazione di etichette semantiche (classi) ai singoli pixel dell'immagine, consentendo così una suddivisione dettagliata dell'immagine in regioni semantiche.

**Batch Normalization:** La batch normalization è una tecnica utilizzata nella segmentazione semantica per migliorare la stabilità e l'efficienza dell'addestramento delle reti neurali. Consiste nell'applicare una normalizzazione statistica ai dati in ciascun batch durante l'addestramento. In pratica, calcola la media e la deviazione standard dei dati in un batch e le utilizza per normalizzare i dati stessi, in modo che abbiano una distribuzione più stabile. Questo accelera la convergenza del modello durante l'addestramento. Nel contesto della segmentazione semantica, la batch normalization può contribuire a ottenere mappe di segmentazione più accurate e a ridurre il rischio di overfitting.

**ReLU (Rectified Linear Activation):** ReLU è una funzione di attivazione comunemente utilizzata nella segmentazione semantica e nelle reti neurali. Restituisce l'input se è positivo o zero, altrimenti restituisce zero. È usata per l'introduzione di non linearità nei modelli. Nella segmentazione semantica, la ReLU aiuta il modello a imparare e catturare caratteristiche semantiche nelle immagini per identificare oggetti e classi di interesse.

**Max Pooling:**: Il max pooling è una tecnica di sottocampionamento comunemente utilizzata nella segmentazione semantica e nelle reti neurali convoluzionali. Consiste nel dividere un'immagine in piccole regioni (spesso chiamate "pooling windows") e selezionare il valore massimo all'interno di ciascuna regione. Questo valore massimo viene utilizzato per creare una nuova immagine a risoluzione inferiore, riducendo così la dimensione dei dati. Il max pooling è utile per ridurre la complessità computazionale e il sovraccampionamento, contribuendo a mantenere eccessivi dettagli spaziali. Nella segmentazione semantica, il max pooling può essere utilizzato nelle fasi di "downsampling" per ridurre progressivamente la dimensione dell'immagine, consentendo al modello di apprendere feature di livello superiore su regioni più ampie.

**Up Sampling:** L'upsampling è una tecnica utilizzata nella segmentazione semantica per aumentare la risoluzione di un'immagine o di una mappa delle caratteristiche. Consiste nell'aumentare le dimensioni dell'immagine attraverso l'interpolazione dei pixel esistenti, consentendo di ottenere mappe di previsioni alla stessa risoluzione dell'immagine originale. Questo è importante per assegnare etichette a ciascun pixel dell'immagine, creando una mappa di segmentazione dettagliata. Una comune tecnica di upsampling è l'upsample bilineare, che interpola i valori dei pixel per riempire lo spazio tra di essi e ottenere un'immagine ad alta risoluzione.

**Transpose Convolution:** La transpose convolution è una tecnica utilizzata nella segmentazione semantica per aumentare la risoluzione delle mappe delle caratteristiche. Funziona aggiungendo pixel vuoti tra i pixel esistenti e applicando pesi ai pixel originali per riempire lo spazio, consentendo di generare mappe di segmentazione dettagliate a una risoluzione più alta rispetto all'input. È ampiamente utilizzata nei livelli di "up-sampling" delle reti neurali convoluzionali per ripristinare la risoluzione dell'immagine alle dimensioni originali o desiderate.
"""

class DoubleConv(nn.Module):
    # (Convolution => BatchNorm => ReLU) * 2
    def __init__(self, in_channels, out_channels, mid_channels=None):
        super().__init__()
        if not mid_channels:
            mid_channels = out_channels
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)

class Down(nn.Module):
    # Downscaling with maxpool then double conv
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(in_channels, out_channels)
        )

    def forward(self, x):
        return self.maxpool_conv(x)

class Up(nn.Module):
    # Upscaling then double conv
    def __init__(self, in_channels, out_channels, bilinear=True):
        super().__init__()
        # if bilinear, use the normal convolutions to reduce the number of channels
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=True)
            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)
        else:
            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv(in_channels, out_channels)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        # input is CHW
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]

        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
                        diffY // 2, diffY - diffY // 2])

        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)

class OutConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(OutConv, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        return self.conv(x)

"""###### U-Net"""

class UNet(nn.Module):
    def __init__(self, n_channels, n_classes, bilinear=False):
        super(UNet, self).__init__()
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = bilinear

        self.inc = (DoubleConv(n_channels, 64))
        self.down1 = (Down(64, 128))
        self.down2 = (Down(128, 256))
        self.down3 = (Down(256, 512))
        factor = 2 if bilinear else 1
        self.down4 = (Down(512, 1024 // factor))
        self.up1 = (Up(1024, 512 // factor, bilinear))
        self.up2 = (Up(512, 256 // factor, bilinear))
        self.up3 = (Up(256, 128 // factor, bilinear))
        self.up4 = (Up(128, 64, bilinear))
        self.outc = (OutConv(64, n_classes))

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        out = self.outc(x)
        return out

"""###### Istanza del modello baseline"""

# Classes number
num_classes = 6

# UNet model
base_model = UNet(n_channels=4, n_classes=6).to(device)

# Loss function
base_criterion = nn.CrossEntropyLoss(ignore_index=-1)

# Optimizer
base_optimizer = optim.Adam(base_model.parameters(), lr=0.001, weight_decay=0.0001)

"""###### Istanza del modello con augmentation"""

# UNet model
aug_model = UNet(n_channels=4, n_classes=6).to(device)

# Loss function
aug_criterion = nn.CrossEntropyLoss(ignore_index=-1)

# Optimizer
aug_optimizer = optim.Adam(aug_model.parameters(), lr=0.001, weight_decay=0.0001)

"""###### Istanza del modello con augmentation e pesi"""

# Weights vector
weights = [1 / w0, 1 / w1, 1 / w2, 1 / w3, 1 / w4, 1 / w5]
class_weights = torch.tensor(weights).to(device)

# UNet model
model = UNet(n_channels=4, n_classes=6).to(device)

# Loss function
criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)

# Optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)   # weight_decay adds a L2 (Ridge) regularization

"""## Passo 7. Addestramento e Validazione

###### Training function
"""

def train_epoch(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss_train = 0.0

    for images, labels in tqdm(train_loader, desc="TRAINING", leave=False, colour="red"):
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels.squeeze(1).long())
        loss.backward()
        optimizer.step()

        total_loss_train += loss.item()

    print("Train Loss: ", total_loss_train / len(train_loader))

    return total_loss_train / len(train_loader)

"""###### Validation function"""

def validate_epoch(model, val_loader, criterion, best_val_loss, path, device):
    model.eval()
    total_loss_val = 0.0
    iou_scores = []
    total_intersection = 0  # NEW
    total_union = 0 # NEW
    total_correct_pixels = 0
    total_labeled_pixels = 0

    predic_labels = []
    true_labels = []
    out_list = []

    with torch.no_grad():
        for images, labels in tqdm(val_loader, desc="VALIDATION", leave=False, colour="yellow"):
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels.squeeze(1).long())
            total_loss_val += loss.item()

            pred = torch.argmax(outputs, dim=1)
            pred_np = pred.cpu().numpy()
            labels_np = labels.cpu().numpy()

            predic_labels.extend(pred.cpu())
            true_labels.extend(labels.cpu())
            out_list.extend(outputs.cpu())

            for i in range(len(pred_np)):
                intersection, union = compute_intersection_over_union(pred_np[i], labels_np[i])
                total_intersection += intersection  # NEW
                total_union += union    # NEW
                iou = compute_iou(intersection, union)
                iou_scores.append(iou)

                pixel_correct, pixel_labeled = compute_pixel_accuracy(pred_np[i], labels_np[i])
                total_correct_pixels += pixel_correct
                total_labeled_pixels += pixel_labeled

    val_loss = total_loss_val / len(val_loader)
    class_wise_iou = np.nanmean(iou_scores, axis=0)
    mean_iou = np.mean(iou_scores)
    pixel_accuracy = compute_pix_acc(total_correct_pixels, total_labeled_pixels)

    batch_iou = compute_iou(total_intersection, total_union)    # NEW
    mean_batch_iou = np.mean(batch_iou) # NEW

    # Save best model weights based on validation loss
    if (total_loss_val / len(val_loader)) < best_val_loss:
        best_val_loss = total_loss_val / len(val_loader)
        torch.save(model.state_dict(), path)    # best_checkpoint_path
        print("Saved best model checkpoint")

    for i in range(len(true_labels)):
        true_labels[i] = true_labels[i].numpy()
    for i in range(len(predic_labels)):
        predic_labels[i] = predic_labels[i].numpy()

    predic_labels = np.array(predic_labels)
    predic_labels = np.squeeze(predic_labels)
    true_labels = np.array(true_labels)
    true_labels = np.squeeze(true_labels)

    predic_labels = np.array(predic_labels).flatten()
    true_labels = np.array(true_labels).flatten()

    cm = confusion_matrix(true_labels, predic_labels)
    iou_conf = confusion_iou(cm)
    iou_conf_mean = np.mean(iou_conf)

    # print("Validation Loss: ", val_loss)
    # print("Class Wise IoU: ", class_wise_iou)
    # print("Mean IoU: ", mean_iou)
    # print("Conf IoU: ", iou_conf)
    # print("Mean Conf IoU: ", iou_conf_mean)

    ax = plt.subplot()
    sns.heatmap(cm, annot=True, fmt='.1f', linewidths=0.5, ax=ax)
    ax.set_xlabel("Predicted Labels")
    ax.set_ylabel("True Labels")
    ax.set_title("Confusion Matrix Validation")
    ax.xaxis.set_ticklabels(CLASS_NAMES[:6])
    ax.yaxis.set_ticklabels(CLASS_NAMES[:6])
    plt.show()

    return val_loss, class_wise_iou, mean_iou, iou_conf, iou_conf_mean, batch_iou, mean_batch_iou, pixel_accuracy, best_val_loss

"""###### Visualize function"""

def visualize_sample(model, val_set, device):
    with torch.no_grad():
        sample_index = np.random.choice(len(val_set), replace=False)
        sample_images, sample_labels = val_set[sample_index]
        sample_images = sample_images.unsqueeze(0).to(device)
        sample_outputs = model(sample_images)

        sample_image = sample_images[0].cpu().squeeze().numpy()
        sample_labels = torch.unsqueeze(sample_labels, 0)
        sample_label = sample_labels.cpu().numpy()
        sample_output = sample_outputs[0].cpu().argmax(dim=0).numpy()

        fig, axes = plt.subplots(1, 3, figsize=(10, 4))
        sample_image_np = undo_normalize_scale(sample_image.transpose(1, 2, 0))
        sample_image_np = sample_image_np[:, :, [2, 1, 0]]  # RGB (remove for BGRN)
        axes[0].imshow(sample_image_np)
        axes[0].set_title("Image")
        axes[1].imshow(sample_label.transpose(1, 2, 0))
        axes[1].set_title("Ground Truth")
        axes[2].imshow(sample_output)
        axes[2].set_title("Prediction")
        plt.show()
        plt.close(fig)

"""###### Train and Validate baseline"""

base_epochs_done = 0
base_num_epochs = 40
base_best_val_loss = float('inf')

base_train_losses = []
base_val_losses = []
base_val_pixel_accuracies = []
base_tot_mean_iou = []
base_tot_mean_conf_iou = []
base_tot_mean_batch_iou = []

base_scheduler = optim.lr_scheduler.StepLR(base_optimizer, step_size=10, gamma=0.1)

for epoch in range(base_epochs_done, base_epochs_done + base_num_epochs):
    # Training phase
    base_train_loss = train_epoch(base_model, no_aug_train_loader, base_optimizer, base_criterion, device)
    base_train_losses.append(base_train_loss)

    # Validation phase
    base_val_loss, base_class_wise_iou, base_mean_iou, base_iou_conf, base_mean_iou_conf, base_batch_iou, base_mean_batch_iou, base_pixel_acc, base_best_val_loss = validate_epoch(base_model, val_loader, base_criterion, base_best_val_loss, base_best_checkpoint_path, device)
    base_val_losses.append(base_val_loss)
    base_val_pixel_accuracies.append(base_pixel_acc)
    base_tot_mean_iou.append(base_mean_iou)
    base_tot_mean_conf_iou.append(base_mean_iou_conf)
    base_tot_mean_batch_iou.append(base_mean_batch_iou)

    # Stampa le metriche di addestramento e validazione per l'epoch corrente
    print(f"Epoch [{epoch + 1}/{base_epochs_done + base_num_epochs}], Class wise IoU validation: {base_class_wise_iou}")
    print(f"Epoch [{epoch + 1}/{base_epochs_done + base_num_epochs}], Confusion IoU validation: {base_iou_conf}")
    print(f"Epoch [{epoch + 1}/{base_epochs_done + base_num_epochs}], Batch IoU validation: {base_batch_iou}")
    print(f"Epoch [{epoch + 1}/{base_epochs_done + base_num_epochs}], Mean IoU Validation: {base_mean_iou:.4f}")
    print(f"Epoch [{epoch + 1}/{base_epochs_done + base_num_epochs}], Confusion Mean IoU Validation: {base_mean_iou_conf:.4f}")
    print(f"Epoch [{epoch + 1}/{base_epochs_done + base_num_epochs}], Batch Mean IoU Validation: {base_mean_batch_iou:.4f}")
    print(f"Epoch [{epoch + 1}/{base_epochs_done + base_num_epochs}], Pixel Accuracy Validation: {base_pixel_acc:.4f}")
    print(f"Epoch [{epoch + 1}/{base_epochs_done + base_num_epochs}], Loss Train: {(base_train_loss):.4f}, Loss Validation: {(base_val_loss):.4f}")

    # Save checkpoint for this epoch
    base_checkpoint = {
        'epoch': epoch,
        'model_state_dict': base_model.state_dict(),
        'optimizer_state_dict': base_optimizer.state_dict(),
        'best_val_loss': base_best_val_loss,
        'train_losses': base_train_losses,
        'val_losses': base_val_losses,
        'val_pixel_accuracies': base_val_pixel_accuracies,
        'mean_iou': base_tot_mean_iou,
        'mean_conf_iou': base_tot_mean_conf_iou,
        'mean_batch_iou': base_tot_mean_batch_iou
    }
    torch.save(base_checkpoint, os.path.join(base_model_checkpoint_path, f"base_checkpoint_epoch_{epoch + 1}.pth"))
    print("Saved checkpoint for epoch ", epoch + 1)

    visualize_sample(base_model, val_set, device)

    base_scheduler.step()

    # epochs_done += 1

base_epochs_done += base_num_epochs

# Save the trained model
torch.save(base_model.state_dict(), base_saved_model_path)

# Plotg loss and accuracy curves
plt.figure(figsize=(10, 5))
plt.plot(range(1, base_epochs_done + 1), base_train_losses, label='Train Loss')
plt.plot(range(1, base_epochs_done + 1), base_val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train and Val Loss')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(range(1, base_epochs_done + 1), base_val_pixel_accuracies, label='Val Pixel Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Validation Pixel Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(range(1, base_epochs_done + 1), base_tot_mean_iou, label="Pixel Mean IoU")
plt.plot(range(1, base_epochs_done + 1), base_tot_mean_conf_iou, label="Confusion Mean IoU")
plt.plot(range(1, base_epochs_done + 1), base_tot_mean_batch_iou, label="Batch Mean IoU")
plt.xlabel('Epoch')
plt.ylabel('IoU')
plt.title('Intersection over Union')
plt.legend()
plt.show()

"""###### Train and validate with augmentations"""

aug_epochs_done = 0
aug_num_epochs = 40
aug_best_val_loss = float('inf')

aug_train_losses = []
aug_val_losses = []
aug_val_pixel_accuracies = []
aug_tot_mean_iou = []
aug_tot_mean_conf_iou = []
aug_tot_mean_batch_iou = []

aug_scheduler = optim.lr_scheduler.StepLR(aug_optimizer, step_size=10, gamma=0.1)

for epoch in range(aug_epochs_done, aug_epochs_done + aug_num_epochs):
    # Training phase
    aug_train_loss = train_epoch(aug_model, train_loader, aug_optimizer, aug_criterion, device)
    aug_train_losses.append(aug_train_loss)

    # Validation phase
    aug_val_loss, aug_class_wise_iou, aug_mean_iou, aug_iou_conf, aug_mean_iou_conf, aug_batch_iou, aug_mean_batch_iou, aug_pixel_acc, aug_best_val_loss = validate_epoch(aug_model, val_loader, aug_criterion, aug_best_val_loss, aug_best_checkpoint_path, device)
    aug_val_losses.append(aug_val_loss)
    aug_val_pixel_accuracies.append(aug_pixel_acc)
    aug_tot_mean_iou.append(aug_mean_iou)
    aug_tot_mean_conf_iou.append(aug_mean_iou_conf)
    aug_tot_mean_batch_iou.append(aug_mean_batch_iou)

    # Stampa le metriche di addestramento e validazione per l'epoch corrente
    print(f"Epoch [{epoch + 1}/{aug_epochs_done + aug_num_epochs}], Class wise IoU validation: {aug_class_wise_iou}")
    print(f"Epoch [{epoch + 1}/{aug_epochs_done + aug_num_epochs}], Confusion IoU validation: {aug_iou_conf}")
    print(f"Epoch [{epoch + 1}/{aug_epochs_done + aug_num_epochs}], Batch IoU validation: {aug_batch_iou}")
    print(f"Epoch [{epoch + 1}/{aug_epochs_done + aug_num_epochs}], Mean IoU Validation: {aug_mean_iou:.4f}")
    print(f"Epoch [{epoch + 1}/{aug_epochs_done + aug_num_epochs}], Confusion Mean IoU Validation: {aug_mean_iou_conf:.4f}")
    print(f"Epoch [{epoch + 1}/{aug_epochs_done + aug_num_epochs}], Batch Mean IoU Validation: {aug_mean_batch_iou:.4f}")
    print(f"Epoch [{epoch + 1}/{aug_epochs_done + aug_num_epochs}], Pixel Accuracy Validation: {aug_pixel_acc:.4f}")
    print(f"Epoch [{epoch + 1}/{aug_epochs_done + aug_num_epochs}], Loss Train: {(aug_train_loss):.4f}, Loss Validation: {(aug_val_loss):.4f}")

    # Save checkpoint for this epoch
    aug_checkpoint = {
        'epoch': epoch,
        'model_state_dict': aug_model.state_dict(),
        'optimizer_state_dict': aug_optimizer.state_dict(),
        'best_val_loss': aug_best_val_loss,
        'train_losses': aug_train_losses,
        'val_losses': aug_val_losses,
        'val_pixel_accuracies': aug_val_pixel_accuracies,
        'mean_iou': aug_tot_mean_iou,
        'mean_conf_iou': aug_tot_mean_conf_iou,
        'mean_batch_iou': aug_tot_mean_batch_iou
    }
    torch.save(aug_checkpoint, os.path.join(aug_model_checkpoint_path, f"aug_checkpoint_epoch_{epoch + 1}.pth"))
    print("Saved checkpoint for epoch ", epoch + 1)

    visualize_sample(aug_model, val_set, device)

    aug_scheduler.step()

    # epochs_done += 1

aug_epochs_done += aug_num_epochs

# Save the trained model
torch.save(aug_model.state_dict(), aug_saved_model_path)

# Plotg loss and accuracy curves
plt.figure(figsize=(10, 5))
plt.plot(range(1, aug_epochs_done + 1), aug_train_losses, label='Train Loss')
plt.plot(range(1, aug_epochs_done + 1), aug_val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train and Val Loss')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(range(1, aug_epochs_done + 1), aug_val_pixel_accuracies, label='Val Pixel Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Validation Pixel Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(range(1, aug_epochs_done + 1), aug_tot_mean_iou, label="Pixel Mean IoU")
plt.plot(range(1, aug_epochs_done + 1), aug_tot_mean_conf_iou, label="Confusion Mean IoU")
plt.plot(range(1, aug_epochs_done + 1), aug_tot_mean_batch_iou, label="Batch Mean IoU")
plt.xlabel('Epoch')
plt.ylabel('IoU')
plt.title('Intersection over Union')
plt.legend()
plt.show()

"""###### Train and validation with augmentations and weights"""

epochs_done = 0
num_epochs = 40
best_val_loss = float('inf')

train_losses = []
val_losses = []
val_pixel_accuracies = []
tot_mean_iou = []
tot_mean_conf_iou = []
tot_mean_batch_iou = []

scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

for epoch in range(epochs_done, epochs_done + num_epochs):
    # Training phase
    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
    train_losses.append(train_loss)

    # Validation phase
    val_loss, class_wise_iou, mean_iou, iou_conf, mean_iou_conf, batch_iou, mean_batch_iou, pixel_acc, best_val_loss = validate_epoch(model, val_loader, criterion, best_val_loss, best_checkpoint_path, device)
    val_losses.append(val_loss)
    val_pixel_accuracies.append(pixel_acc)
    tot_mean_iou.append(mean_iou)
    tot_mean_conf_iou.append(mean_iou_conf)
    tot_mean_batch_iou.append(mean_batch_iou)

    # Stampa le metriche di addestramento e validazione per l'epoch corrente
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Class wise IoU validation: {class_wise_iou}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Confusion IoU validation: {iou_conf}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Batch IoU validation: {batch_iou}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Mean IoU Validation: {mean_iou:.4f}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Confusion Mean IoU Validation: {mean_iou_conf:.4f}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Batch Mean IoU Validation: {mean_batch_iou:.4f}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Pixel Accuracy Validation: {pixel_acc:.4f}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Loss Train: {(train_loss):.4f}, Loss Validation: {(val_loss):.4f}")

    # Save checkpoint for this epoch
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_val_loss': best_val_loss,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'val_pixel_accuracies': val_pixel_accuracies,
        'mean_iou': tot_mean_iou,
        'mean_conf_iou': tot_mean_conf_iou,
        'mean_batch_iou': tot_mean_batch_iou
    }
    torch.save(checkpoint, os.path.join(model_checkpoint_path, f"checkpoint_epoch_{epoch + 1}.pth"))
    print("Saved checkpoint for epoch ", epoch + 1)

    visualize_sample(model, val_set, device)

    scheduler.step()

    # epochs_done += 1

epochs_done += num_epochs

# Save the trained model
torch.save(model.state_dict(), saved_model_path)

# Plotg loss and accuracy curves
plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs_done + 1), train_losses, label='Train Loss')
plt.plot(range(1, epochs_done + 1), val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train and Val Loss')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs_done + 1), val_pixel_accuracies, label='Val Pixel Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Validation Pixel Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs_done + 1), tot_mean_iou, label="Pixel Mean IoU")
plt.plot(range(1, epochs_done + 1), tot_mean_conf_iou, label="Confusion Mean IoU")
plt.plot(range(1, epochs_done + 1), tot_mean_batch_iou, label="Batch Mean IoU")
plt.xlabel('Epoch')
plt.ylabel('IoU')
plt.title('Intersection over Union')
plt.legend()
plt.show()

# Load checkpoints if available
if os.path.exists(model_checkpoint_path):
    # checkpoint = torch.load(os.path.join(model_checkpoint_path, f"checkpoint_epoch_{checkpoint['epoch'] + 1}.pth"))
    checkpoint = torch.load(os.path.join(model_checkpoint_path, "checkpoint_epoch_40.pth"))
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    # best_val_loss = checkpoint['best_val_loss']
    best_val_loss = 0.9755
    train_losses = checkpoint['train_losses']
    val_losses = checkpoint['val_losses']
    val_pixel_accuracies = checkpoint['val_pixel_accuracies']
    tot_mean_iou = checkpoint['mean_iou']
    tot_mean_conf_iou = checkpoint['mean_conf_iou']
    tot_mean_batch_iou = checkpoint['mean_batch_iou']

    print("Loaded checkpoint from epoch ", checkpoint['epoch'] + 1)

epochs_done = 40
num_epochs = 40

scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)

for epoch in range(epochs_done, epochs_done + num_epochs):
    # Training phase
    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
    train_losses.append(train_loss)

    # Validation phase
    val_loss, class_wise_iou, mean_iou, iou_conf, mean_iou_conf, batch_iou, mean_batch_iou, pixel_acc, best_val_loss = validate_epoch(model, val_loader, criterion, best_val_loss, best_checkpoint_path, device)
    val_losses.append(val_loss)
    val_pixel_accuracies.append(pixel_acc)
    tot_mean_iou.append(mean_iou)
    tot_mean_conf_iou.append(mean_iou_conf)
    tot_mean_batch_iou.append(mean_batch_iou)

    # Stampa le metriche di addestramento e validazione per l'epoch corrente
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Class wise IoU validation: {class_wise_iou}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Confusion IoU validation: {iou_conf}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Batch IoU validation: {batch_iou}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Mean IoU Validation: {mean_iou:.4f}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Confusion Mean IoU Validation: {mean_iou_conf:.4f}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Batch Mean IoU Validation: {mean_batch_iou:.4f}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Pixel Accuracy Validation: {pixel_acc:.4f}")
    print(f"Epoch [{epoch + 1}/{epochs_done + num_epochs}], Loss Train: {(train_loss):.4f}, Loss Validation: {(val_loss):.4f}")

    # Save checkpoint for this epoch
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_val_loss': best_val_loss,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'val_pixel_accuracies': val_pixel_accuracies,
        'mean_iou': tot_mean_iou,
        'mean_conf_iou': tot_mean_conf_iou,
        'mean_batch_iou': tot_mean_batch_iou
    }
    torch.save(checkpoint, os.path.join(model_checkpoint_path, f"checkpoint_epoch_{epoch + 1}.pth"))
    print("Saved checkpoint for epoch ", epoch + 1)

    visualize_sample(model, val_set, device)

    scheduler.step()

    # epochs_done += 1

epochs_done += num_epochs

# Save the trained model
torch.save(model.state_dict(), saved_model_path)

# Plotg loss and accuracy curves
plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs_done + 1), train_losses, label='Train Loss')
plt.plot(range(1, epochs_done + 1), val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train and Val Loss')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs_done + 1), val_pixel_accuracies, label='Val Pixel Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Validation Pixel Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs_done + 1), tot_mean_iou, label="Pixel Mean IoU")
plt.plot(range(1, epochs_done + 1), tot_mean_conf_iou, label="Confusion Mean IoU")
plt.plot(range(1, epochs_done + 1), tot_mean_batch_iou, label="Batch Mean IoU")
plt.xlabel('Epoch')
plt.ylabel('IoU')
plt.title('Intersection over Union')
plt.legend()
plt.show()

# Plotg loss and accuracy curves
plt.figure(figsize=(10, 5))
plt.plot(range(1, 71), train_losses, label='Train Loss')
plt.plot(range(1, 70), val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train and Val Loss')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(range(1, 70), val_pixel_accuracies, label='Val Pixel Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Validation Pixel Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(range(1, 70), tot_mean_iou, label="Pixel Mean IoU")
plt.plot(range(1, 70), tot_mean_conf_iou, label="Confusion Mean IoU")
plt.plot(range(1, 70), tot_mean_batch_iou, label="Batch Mean IoU")
plt.xlabel('Epoch')
plt.ylabel('IoU')
plt.title('Intersection over Union')
plt.legend()
plt.show()

"""## Passo 8. Test"""

def test(model, test_loader, device):
    model.eval()
    iou_scores = []
    total_intersection = 0
    total_union = 0
    total_correct_pixels = 0
    total_labeled_pixels = 0

    predic_labels = []
    true_labels = []
    out_list = []

    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="TEST", leave=False, colour="green"):
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)

            pred = torch.argmax(outputs, dim=1)
            pred_np = pred.cpu().numpy()
            labels_np = labels.cpu().numpy()

            predic_labels.extend(pred.cpu())
            true_labels.extend(labels.cpu())
            out_list.extend(outputs.cpu())

            for i in range(len(pred_np)):
                intersection, union = compute_intersection_over_union(pred_np[i], labels_np[i])
                total_intersection += intersection  # NEW
                total_union += union    # NEW
                iou = compute_iou(intersection, union)
                iou_scores.append(iou)

                pixel_correct, pixel_labeled = compute_pixel_accuracy(pred_np[i], labels_np[i])
                total_correct_pixels += pixel_correct
                total_labeled_pixels += pixel_labeled

    class_wise_iou = np.nanmean(iou_scores, axis=0)
    mean_iou = np.mean(iou_scores)
    pixel_accuracy = compute_pix_acc(total_correct_pixels, total_labeled_pixels)

    batch_iou = compute_iou(total_intersection, total_union)    # NEW
    mean_batch_iou = np.mean(batch_iou) # NEW

    for i in range(len(true_labels)):
        true_labels[i] = true_labels[i].numpy()
    for i in range(len(predic_labels)):
        predic_labels[i] = predic_labels[i].numpy()

    predic_labels = np.array(predic_labels)
    predic_labels = np.squeeze(predic_labels)
    true_labels = np.array(true_labels)
    true_labels = np.squeeze(true_labels)

    predic_labels = np.array(predic_labels).flatten()
    true_labels = np.array(true_labels).flatten()

    cm = confusion_matrix(true_labels, predic_labels)
    iou_conf = confusion_iou(cm)
    iou_conf_mean = np.mean(iou_conf)

    ax = plt.subplot()
    sns.heatmap(cm, annot=True, fmt='.1f', linewidths=0.5, ax=ax)
    ax.set_xlabel("Predicted Labels")
    ax.set_ylabel("True Labels")
    ax.set_title("Confusion Matrix Test")
    ax.xaxis.set_ticklabels(CLASS_NAMES[:6])
    ax.yaxis.set_ticklabels(CLASS_NAMES[:6])
    plt.show()

    return class_wise_iou, mean_iou, iou_conf, iou_conf_mean, batch_iou, mean_batch_iou, pixel_accuracy

"""###### Test su baseline"""

# Load the best model's weights
base_best_model = UNet(n_channels=4, n_classes=6).to(device)
base_best_model.load_state_dict(torch.load(base_best_checkpoint_path))

base_test_iou, base_test_mean_iou, base_test_iou_conf, base_test_iou_conf_mean, base_test_batch_iou, base_test_mean_batch_iou, base_test_pix_acc = test(base_best_model, test_loader, device)
visualize_sample(base_best_model, test_set, device)

# Stampa le metriche di test
print(f"Class wise IoU test:    {base_test_iou}")
print(f"Confusion IoU test:     {base_test_iou_conf}")
print(f"Batch IoU test:         {base_test_batch_iou}")
print(f"Mean IoU test:          {base_test_mean_iou:.4f}")
print(f"Confusion Mean IoU test:{base_test_iou_conf_mean:.4f}")
print(f"Batch Mean IoU test:    {base_test_mean_batch_iou:.4f}")
print(f"Pixel Accuracy test:    {base_test_pix_acc:.4f}")

"""###### Test with augmentations"""

# Load the best model's weights
aug_best_model = UNet(n_channels=4, n_classes=6).to(device)
aug_best_model.load_state_dict(torch.load(aug_best_checkpoint_path))

aug_test_iou, aug_test_mean_iou, aug_test_iou_conf, aug_test_iou_conf_mean, aug_test_batch_iou, aug_test_mean_batch_iou, aug_test_pix_acc = test(aug_best_model, test_loader, device)
visualize_sample(aug_best_model, test_set, device)

# Stampa le metriche di test
print(f"Class wise IoU test:    {aug_test_iou}")
print(f"Confusion IoU test:     {aug_test_iou_conf}")
print(f"Batch IoU test:         {aug_test_batch_iou}")
print(f"Mean IoU test:          {aug_test_mean_iou:.4f}")
print(f"Confusion Mean IoU test:{aug_test_iou_conf_mean:.4f}")
print(f"Batch Mean IoU test:    {aug_test_mean_batch_iou:.4f}")
print(f"Pixel Accuracy test:    {aug_test_pix_acc:.4f}")

"""###### Test with augmentations and weights"""

# Load the best model's weights
best_model = UNet(n_channels=4, n_classes=6).to(device)
# best_model.load_state_dict(torch.load(best_checkpoint_path))    # SEARCH AND LOAD BEST CHECKPOINT; BEST MODEL IS WRONG!!!
checkpoint_best = torch.load(os.path.join(model_checkpoint_path, "checkpoint_epoch_19.pth"))
best_model.load_state_dict(checkpoint_best['model_state_dict'])

test_iou, test_mean_iou, test_iou_conf, test_iou_conf_mean, test_batch_iou, test_mean_batch_iou, test_pix_acc = test(best_model, test_loader, device)
visualize_sample(best_model, test_set, device)

# Stampa le metriche di test
print(f"Class wise IoU test:    {test_iou}")
print(f"Confusion IoU test:     {test_iou_conf}")
print(f"Batch IoU test:         {test_batch_iou}")
print(f"Mean IoU test:          {test_mean_iou:.4f}")
print(f"Confusion Mean IoU test:{test_iou_conf_mean:.4f}")
print(f"Batch Mean IoU test:    {test_mean_batch_iou:.4f}")
print(f"Pixel Accuracy test:    {test_pix_acc:.4f}")

# Load the best model's weights
best_model_2 = UNet(n_channels=4, n_classes=6).to(device)
# best_model.load_state_dict(torch.load(best_checkpoint_path))    # SEARCH AND LOAD BEST CHECKPOINT; BEST MODEL IS WRONG!!!
checkpoint_best_2 = torch.load(os.path.join(model_checkpoint_path, "checkpoint_epoch_42.pth"))
best_model_2.load_state_dict(checkpoint_best_2['model_state_dict'])

test_2_iou, test_2_mean_iou, test_2_iou_conf, test_2_iou_conf_mean, test_2_batch_iou, test_2_mean_batch_iou, test_2_pix_acc = test(best_model_2, test_loader, device)
visualize_sample(best_model_2, test_set, device)

# Stampa le metriche di test
print(f"Class wise IoU test:    {test_2_iou}")
print(f"Confusion IoU test:     {test_2_iou_conf}")
print(f"Batch IoU test:         {test_2_batch_iou}")
print(f"Mean IoU test:          {test_2_mean_iou:.4f}")
print(f"Confusion Mean IoU test:{test_2_iou_conf_mean:.4f}")
print(f"Batch Mean IoU test:    {test_2_mean_batch_iou:.4f}")
print(f"Pixel Accuracy test:    {test_2_pix_acc:.4f}")